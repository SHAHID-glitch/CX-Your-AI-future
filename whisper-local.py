import whisper
import torch

print("ğŸ¤ Loading Whisper model...")
# Use tiny model for speed (works without GPU)
model = whisper.load_model("tiny")

audio_file = "voice.wav"
print(f"ğŸ“ Processing: {audio_file}")

# Transcribe
print("â³ Transcribing...")
result = model.transcribe(audio_file, fp16=False)  # fp16=False for CPU

print("\nâœ… Transcription complete!\n")
print("ğŸ“ Text:", result["text"])
print("\n" + "="*50)

# Save to file
with open("transcription.txt", "w", encoding="utf-8") as f:
    f.write(result["text"])
    
print("ğŸ’¾ Saved to: transcription.txt")
